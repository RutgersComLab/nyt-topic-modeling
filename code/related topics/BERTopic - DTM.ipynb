{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Setup Environment](#section1)\n",
    "* [Data](#section2)\n",
    "* [Basic Topic Model](#section3)\n",
    "* [Extracting Topics](#section4)\n",
    "* [Topics over Time](#section5)\n",
    "* [Visualize Topics over Time](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment <a class=anchor id=section1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get update\n",
    "!apt-get install --reinstall build-essential --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/MaartenGr/BERTopic.git@407fd4fdf2e05e80019c1c217972bf3314a41040\n",
    "!pip install farm-haystack\n",
    "!pip install spacy\n",
    "!pip install gensim\n",
    "!pip install sagemaker_pyspark\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "ERROR - root -  Failed to import 'magic' (from 'python-magic' and 'python-magic-bin' on Windows). FileTypeClassifier will not perform mimetype detection on extensionless files. Please make sure the necessary OS libraries are installed if you need this functionality.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from haystack.nodes import PreProcessor\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from haystack.utils import convert_files_to_docs\n",
    "\n",
    "pio.renderers.default='iframe'\n",
    "logging.getLogger(\"haystack.utils.preprocessing\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t)\n",
    "            for t in word_tokenize(doc)\n",
    "            if len(t) >= 2 and re.match(\"[a-z].*\", t) and re.match(token_pattern, t)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data <a class=anchor id=section2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['CLEANSED/1950-1959', 'CLEANSED/1960-1969', 'CLEANSED/1970-1979', 'CLEANSED/1980-1989', \n",
    "          'CLEANSED/1990-1999', 'CLEANSED/2000-2009', 'CLEANSED/2010-2019', 'CLEANSED/2020-2029']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1950-1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [00:01<00:00, 794.79docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1051\n",
      "Number of output files: 2168\n",
      "Processing 1960-1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2900/2900 [00:02<00:00, 971.53docs/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 2900\n",
      "Number of output files: 5093\n",
      "Processing 1970-1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [00:03<00:00, 341.76docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1258\n",
      "Number of output files: 3589\n",
      "Processing 1980-1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1771 [00:00<?, ?docs/s]WARNING - haystack.nodes.preprocessor.preprocessor -  One or more sentence found with word count higher than the split length.\n",
      "100%|██████████| 1771/1771 [00:01<00:00, 1003.97docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1771\n",
      "Number of output files: 4032\n",
      "Processing 1990-1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 326/1520 [00:00<00:01, 1067.05docs/s]WARNING - haystack.nodes.preprocessor.preprocessor -  One or more sentence found with word count higher than the split length.\n",
      "100%|██████████| 1520/1520 [00:01<00:00, 1065.39docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1520\n",
      "Number of output files: 3447\n",
      "Processing 2000-2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2010/2010 [00:02<00:00, 925.38docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 2010\n",
      "Number of output files: 4989\n",
      "Processing 2010-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2852/2852 [00:03<00:00, 791.48docs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 2852\n",
      "Number of output files: 8442\n",
      "Processing 2020-2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412/412 [00:00<00:00, 543.05docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 412\n",
      "Number of output files: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "timestamps = []\n",
    "for source in sources:\n",
    "    print(\"Processing {}\".format(source.split(\"/\")[1]))\n",
    "    all_docs = convert_files_to_docs(dir_path=source)\n",
    "    preprocessor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=False,\n",
    "        split_by=\"word\",\n",
    "        split_length=500,\n",
    "        split_respect_sentence_boundary=True,)\n",
    "    processed_docs = preprocessor.process(all_docs)\n",
    "    print(f\"Number of input files: {len(all_docs)}\\nNumber of output files: {len(processed_docs)}\")\n",
    "    docs.extend([item.content for item in processed_docs])\n",
    "    timestamps.extend([source.split(\"-\")[1]]*len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33296\n",
      "33296\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(len(timestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Topic Model <a class=anchor id=section3></a>\n",
    "\n",
    "To perform Dynamic Topic Modeling with BERTopic we will first need to create a basic topic model using all articles. The temporal aspect will be ignored as we are, for now, only interested in the topics that reside in those articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7321c512007347f5a50444154d127e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1041.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 06:30:36,328 - BERTopic - Transformed documents to Embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 06:31:38,492 - BERTopic - Reduced dimensionality\n",
      "2022-06-02 06:42:35,189 - BERTopic - Clustered reduced embeddings\n",
      "2022-06-02 06:50:05,851 - BERTopic - Reduced number of topics from 487 to 362\n"
     ]
    }
   ],
   "source": [
    "vectorizer_model= CountVectorizer(stop_words=\"english\", tokenizer=LemmaTokenizer())\n",
    "# Set the random state in the UMAP model to prevent stochastic behavior \n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=1)\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True,\n",
    "                       nr_topics=\"auto\", vectorizer_model=vectorizer_model, umap_model=umap_model)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning:\n",
      "\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"bert_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Topics <a class=anchor id=section4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract most frequent topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>11520</td>\n",
       "      <td>-1_wa_said_job_company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>786</td>\n",
       "      <td>0_exchange_trading_stock_market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>719</td>\n",
       "      <td>1_ship_port_cargo_longshoreman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>2_pilot_airline_plane_flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>3_gm_steel_car_ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>577</td>\n",
       "      <td>4_trump_republican_democratic_democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>516</td>\n",
       "      <td>5_hospital_patient_health_doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>428</td>\n",
       "      <td>6_car_selfdriving_vehicle_driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>343</td>\n",
       "      <td>7_ibm_software_microsoft_computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>341</td>\n",
       "      <td>8_student_school_education_college</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                    Name\n",
       "0     -1  11520                  -1_wa_said_job_company\n",
       "1      0    786         0_exchange_trading_stock_market\n",
       "2      1    719          1_ship_port_cargo_longshoreman\n",
       "3      2    608            2_pilot_airline_plane_flight\n",
       "4      3    600                     3_gm_steel_car_ford\n",
       "5      4    577  4_trump_republican_democratic_democrat\n",
       "6      5    516        5_hospital_patient_health_doctor\n",
       "7      6    428        6_car_selfdriving_vehicle_driver\n",
       "8      7    343       7_ibm_software_microsoft_computer\n",
       "9      8    341      8_student_school_education_college"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -1 refers to all outliers and should typically be ignored. \n",
    "\n",
    "Next, let's take a look at a frequent topic that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gm', 0.017653390217818362),\n",
       " ('steel', 0.013759767702444145),\n",
       " ('car', 0.012598195314218553),\n",
       " ('ford', 0.01249254212854796),\n",
       " ('auto', 0.010309827941324125),\n",
       " ('motor', 0.009743642008408592),\n",
       " ('plant', 0.009568833052719059),\n",
       " ('japanese', 0.008050828379352693),\n",
       " ('toyota', 0.007534728555264),\n",
       " ('assembly', 0.006684561486707328)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_nr = freq.iloc[4][\"Topic\"]  # We select a frequent topic\n",
    "topic_model.get_topic(topic_nr)   # You can select a topic number as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the basic topics that were created with the Intertopic Distance Map. This allows us to judge visually whether the basic topics are sufficient before proceeding to creating the topics over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"670px\"\n",
       "    height=\"670\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics over Time <a class=anchor id=section5></a>\n",
    "\n",
    "Before we start with the Dynamic Topic Modeling step, it is important that you are satisfied with the topics that were created previously. We are going to be using those specific topics as a base for Dynamic Topic Modeling. Thus, this step will essentially show you how the topics that were defined previously have evolved over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few important parameters that you should take note of, namely:\n",
    "* `docs`\n",
    "  * These are the articles that we are using\n",
    "* `topics`\n",
    "  * The topics that we have created before\n",
    "* `timestamps`\n",
    "  * The timestamp of each article/document\n",
    "* `global_tuning`\n",
    "  * Whether to average the topic representation of a topic at time *t* with its global topic representation\n",
    "* `evolution_tuning`\n",
    "  * Whether to average the topic representation of a topic at time *t* with the topic representation of that topic at time *t-1*\n",
    "* `nr_bins`\n",
    "  * The number of bins to put our timestamps into. It is computationally inefficient to extract the topics at thousands of different timestamps. Therefore, it is advised to keep this value below 20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33296\n",
      "33296\n",
      "33296\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(len(topics))\n",
    "print(len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:35, 19.38s/it]\n"
     ]
    }
   ],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=docs, \n",
    "                                                topics=topics, \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True, \n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>permission, reproduced, copyright, reproduction, prohibited</td>\n",
       "      <td>971</td>\n",
       "      <td>1958-12-06 10:22:04.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>exchange, stock, margin, walston, trading</td>\n",
       "      <td>13</td>\n",
       "      <td>1958-12-06 10:22:04.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cargo, ship, port, longshoreman, container</td>\n",
       "      <td>79</td>\n",
       "      <td>1958-12-06 10:22:04.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>airline, airport, traffic, pilot, plane</td>\n",
       "      <td>16</td>\n",
       "      <td>1958-12-06 10:22:04.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>steel, ford, mcdonald, union, gm</td>\n",
       "      <td>58</td>\n",
       "      <td>1958-12-06 10:22:04.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>335</td>\n",
       "      <td>helsinki, browne, carbon, kalasatama, neighborhood</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-04-02 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>343</td>\n",
       "      <td>smoke, alarm, nest, detector, smart</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-02 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>353</td>\n",
       "      <td>resister, nettie, aunt, gwen, novel</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-04-02 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>357</td>\n",
       "      <td>medicaid, higherincome, enrolled, benefit, requirement</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-04-02 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>358</td>\n",
       "      <td>giulia, marchi, finite, harm, photograph</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-04-02 00:00:00.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1582 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic                                                        Words  \\\n",
       "0        -1  permission, reproduced, copyright, reproduction, prohibited   \n",
       "1         0                    exchange, stock, margin, walston, trading   \n",
       "2         1                   cargo, ship, port, longshoreman, container   \n",
       "3         2                      airline, airport, traffic, pilot, plane   \n",
       "4         3                             steel, ford, mcdonald, union, gm   \n",
       "...     ...                                                          ...   \n",
       "1577    335           helsinki, browne, carbon, kalasatama, neighborhood   \n",
       "1578    343                          smoke, alarm, nest, detector, smart   \n",
       "1579    353                          resister, nettie, aunt, gwen, novel   \n",
       "1580    357       medicaid, higherincome, enrolled, benefit, requirement   \n",
       "1581    358                     giulia, marchi, finite, harm, photograph   \n",
       "\n",
       "      Frequency               Timestamp  \n",
       "0           971 1958-12-06 10:22:04.800  \n",
       "1            13 1958-12-06 10:22:04.800  \n",
       "2            79 1958-12-06 10:22:04.800  \n",
       "3            16 1958-12-06 10:22:04.800  \n",
       "4            58 1958-12-06 10:22:04.800  \n",
       "...         ...                     ...  \n",
       "1577          3 2020-04-02 00:00:00.000  \n",
       "1578          1 2020-04-02 00:00:00.000  \n",
       "1579         10 2020-04-02 00:00:00.000  \n",
       "1580          7 2020-04-02 00:00:00.000  \n",
       "1581          1 2020-04-02 00:00:00.000  \n",
       "\n",
       "[1582 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Topics over Time <a class=anchor id=section6></a>\n",
    "\n",
    "After having created our `topics_over_time`, we will have to visualize those topics as accessing them becomes a bit more difficult with the added temporal dimension. \n",
    "\n",
    "To do so, we are going to visualize the distribution of topics over time based on their frequency. Doing so allows us to see how the topics have evolved over time. Make sure to hover over any point to see how the topic representation at time *t* differs from the global topic representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1270px\"\n",
       "    height=\"470\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=50, normalize_frequency=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
