{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Talk about (1) and outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import re\n",
    "import json \n",
    "import gzip\n",
    "import glob\n",
    "import nltk\n",
    "import string\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file        \n",
    "# fname = \"ProQuestDocuments-2020-10-160_2017_1857295624\".split(\"_\")[0]\n",
    "# with open('2010-2020 unformatted/'+fname+'.txt', 'r') as f:\n",
    "#     print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Categorize on years (1960-1979)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the text iself has no 'year' section, the logic basically follows that the files mentioned in CSV must be only read. This leads to a missing 90 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r CLEANSED/\n",
    "!mkdir CLEANSED\n",
    "!mkdir CLEANSED/1950-1959/\n",
    "!mkdir CLEANSED/1960-1969/\n",
    "!mkdir CLEANSED/1970-1979/\n",
    "!mkdir CLEANSED/1980-1989/\n",
    "!mkdir CLEANSED/1990-1999/\n",
    "!mkdir CLEANSED/2000-2009/\n",
    "!mkdir CLEANSED/2010-2019/\n",
    "!mkdir CLEANSED/2020-2029/\n",
    "!mkdir CLEANSED/outlier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "Total no.of files in source directory: 4248\n",
      "Total no.of files in 1960-1969 directory: 2900\n",
      "Total no.of files in 1970-1979 directory: 1258\n",
      "Total no.of files in outlier directory: 0\n",
      "Number of missing files: 90\n"
     ]
    }
   ],
   "source": [
    "content = {}\n",
    "counter_1960_1969 = 0\n",
    "counter_1970_1979 = 0\n",
    "counter_1960_1979_outlier = 0\n",
    "files = glob.glob('RAW/1960-1979_dump/*.txt')\n",
    "\n",
    "df = pd.read_csv(\"METADATA/1960-1979.csv\", usecols=['StoreId', 'year'])\n",
    "id_year = dict(zip(df['StoreId'], df['year']))\n",
    "\n",
    "for store_id, year in id_year.items():\n",
    "    for file_id in files:\n",
    "        file_name = file_id.split(\"/\")[-1]\n",
    "        if file_name.startswith(str(store_id)):\n",
    "            with open(file_id, 'r') as f:\n",
    "                data = f.read()\n",
    "            if 1960 <= int(year) <= 1969:\n",
    "                prefix = '1960-1969/'\n",
    "                counter_1960_1969+=1\n",
    "            elif 1970 <= int(year) <= 1979:\n",
    "                prefix = '1970-1979/'\n",
    "                counter_1970_1979+=1\n",
    "            else:\n",
    "                prefix = \"outlier/\"\n",
    "                counter_outlier+=1\n",
    "            \n",
    "            with open(\"CLEANSED/\"+prefix+file_name, 'w') as file:\n",
    "                file.write(data)\n",
    "            break\n",
    "\n",
    "print(\"FINISHED\")\n",
    "print(\"Total no.of files in source directory: {}\".format(len(files)))\n",
    "print(\"Total no.of files in 1960-1969 directory: {}\".format(counter_1960_1969))\n",
    "print(\"Total no.of files in 1970-1979 directory: {}\".format(counter_1970_1979))\n",
    "print(\"Total no.of files in outlier directory: {}\".format(counter_1960_1979_outlier))\n",
    "print(\"Number of missing files: {}\".format(len(files)-(counter_1960_1969+counter_1970_1979+counter_1960_1979_outlier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1980-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = ('title', 'publication title', 'publication year', 'document url', 'links', 'section', 'publication subject', 'issn', 'copyright', 'abstract', 'publication info', 'last updated', 'place of publication', 'location', 'author', 'publisher', 'identifier / keyword', 'source type', 'proquest document id', 'country of publication', 'language of publication', 'publication date', 'subject', 'database', 'document type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Extract text and categorize on years (1980-2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "Total no.of files in source directory: 5141\n",
      "Total no.of files in 1980-1989 directory: 1771\n",
      "Total no.of files in 1990-1999 directory: 1520\n",
      "Total no.of files in 2000-2009 directory: 1850\n",
      "Total no.of files in outlier directory: 0\n",
      "Number of missing files: 0\n"
     ]
    }
   ],
   "source": [
    "content = {}\n",
    "counter_1980_1989 = 0\n",
    "counter_1990_1999 = 0\n",
    "counter_2000_2009 = 0\n",
    "counter_1980_2009_outlier = 0\n",
    "files = glob.glob('RAW/1980-2009_raw/*.txt')\n",
    "\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = \"\"\n",
    "        flag = False\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            if line.startswith('proquest document id:'):\n",
    "                document_id = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('publication year:'):\n",
    "                year = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('full text:'):\n",
    "                flag=True\n",
    "                data=line.split('full text:')[1].replace(\"\\n\", \"\")\n",
    "            elif line.strip().startswith(sections):\n",
    "                flag=False\n",
    "            elif flag:\n",
    "                data+=line.replace(\"\\n\", \"\")\n",
    "    content[file_name.split(\"/\")[-1][:-4] + \"_\" + year + \"_\" + document_id] = data.strip()\n",
    "\n",
    "for key, value in content.items():\n",
    "    file_name, year, document_id = key.split(\"_\")\n",
    "    if 1980<= int(year) <=1989:\n",
    "        prefix = '1980-1989/'\n",
    "        counter_1980_1989+=1\n",
    "    elif 1990<= int(year) <=1999:\n",
    "        prefix = '1990-1999/'\n",
    "        counter_1990_1999+=1\n",
    "    elif 2000<= int(year) <=2009:\n",
    "        prefix = '2000-2009/'\n",
    "        counter_2000_2009+=1\n",
    "    else:\n",
    "        prefix = \"outlier/\"\n",
    "        counter_1980_2009_outlier+=1\n",
    "\n",
    "    with open(\"CLEANSED/\"+prefix+file_name+\"_\"+document_id+'.txt', 'w') as file:\n",
    "        file.write(value)\n",
    "\n",
    "print(\"FINISHED\")\n",
    "print(\"Total no.of files in source directory: {}\".format(len(files)))\n",
    "print(\"Total no.of files in 1980-1989 directory: {}\".format(counter_1980_1989))\n",
    "print(\"Total no.of files in 1990-1999 directory: {}\".format(counter_1990_1999))\n",
    "print(\"Total no.of files in 2000-2009 directory: {}\".format(counter_2000_2009))\n",
    "print(\"Total no.of files in outlier directory: {}\".format(counter_1980_2009_outlier))\n",
    "print(\"Number of missing files: {}\".format(len(files)-(counter_1980_1989+counter_1990_1999+counter_2000_2009+counter_1980_2009_outlier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Extract text for years 2010-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "Total no.of files in source directory: 3012\n",
      "Total no.of files in 2010-2019 directory: 2852\n",
      "Total no.of files in outlier directory: 160\n",
      "Number of missing files: 0\n"
     ]
    }
   ],
   "source": [
    "content = {}\n",
    "counter_2010_2019 = 0\n",
    "counter_2010_2019_outlier = 0\n",
    "files = glob.glob('RAW/2010-2019_raw/*.txt')\n",
    "\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = \"\"\n",
    "        flag = False\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            if line.startswith('proquest document id:'):\n",
    "                document_id = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('publication year:'):\n",
    "                year = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('full text:'):\n",
    "                flag=True\n",
    "                data=line.split('full text:')[1].replace(\"\\n\", \"\")\n",
    "            elif line.strip().startswith(sections):\n",
    "                flag=False\n",
    "            elif flag:\n",
    "                data+=line.replace(\"\\n\", \"\")\n",
    "    content[file_name.split(\"/\")[-1][:-4] + \"_\" + year + \"_\" + document_id] = data.strip()\n",
    "\n",
    "for key, value in content.items():\n",
    "    file_name, year, document_id = key.split(\"_\")\n",
    "    if 2010<= int(year) <=2019:\n",
    "        prefix = '2010-2019/'\n",
    "        counter_2010_2019+=1\n",
    "    else:\n",
    "        prefix = \"outlier/\"\n",
    "        counter_2010_2019_outlier+=1\n",
    "        \n",
    "    with open(\"CLEANSED/\"+prefix+file_name+\"_\"+document_id+'.txt', 'w') as file:\n",
    "        file.write(value)\n",
    "print(\"FINISHED\")\n",
    "print(\"Total no.of files in source directory: {}\".format(len(files)))\n",
    "print(\"Total no.of files in 2010-2019 directory: {}\".format(counter_2010_2019))\n",
    "print(\"Total no.of files in outlier directory: {}\".format(counter_2010_2019_outlier))\n",
    "print(\"Number of missing files: {}\".format(len(files)-(counter_2010_2019+counter_2010_2019_outlier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Extract text for years 2020-2029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "Total no.of files in source directory: 412\n",
      "Total no.of files in 2020-2029 directory: 412\n",
      "Total no.of files in outlier directory: 0\n",
      "Number of missing files: 0\n"
     ]
    }
   ],
   "source": [
    "content = {}\n",
    "counter_2020_2029 = 0\n",
    "counter_2020_2029_outlier = 0\n",
    "files = glob.glob('RAW/2020-2029_raw/*.txt')\n",
    "\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = \"\"\n",
    "        flag = False\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            if line.startswith('proquest document id:'):\n",
    "                document_id = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('publication year:'):\n",
    "                year = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('full text:'):\n",
    "                flag=True\n",
    "                data=line.split('full text:')[1].replace(\"\\n\", \"\")\n",
    "            elif line.strip().startswith(sections):\n",
    "                flag=False\n",
    "            elif flag:\n",
    "                data+=line.replace(\"\\n\", \"\")\n",
    "    content[file_name.split(\"/\")[-1][:-4] + \"_\" + year + \"_\" + document_id] = data.strip()\n",
    "\n",
    "for key, value in content.items():\n",
    "    file_name, year, document_id = key.split(\"_\")\n",
    "    if 2020<= int(year) <=2029:\n",
    "        prefix = '2020-2029/'\n",
    "        counter_2020_2029+=1\n",
    "    else:\n",
    "        prefix = \"outlier/\"\n",
    "        counter_2020_2029_outlier+=1\n",
    "        \n",
    "    with open(\"CLEANSED/\"+prefix+file_name+\"_\"+document_id+'.txt', 'w') as file:\n",
    "        file.write(value)\n",
    "print(\"FINISHED\")\n",
    "print(\"Total no.of files in source directory: {}\".format(len(files)))\n",
    "print(\"Total no.of files in 2020-2029 directory: {}\".format(counter_2020_2029))\n",
    "print(\"Total no.of files in outlier directory: {}\".format(counter_2020_2029_outlier))\n",
    "print(\"Number of missing files: {}\".format(len(files)-(counter_2020_2029+counter_2020_2029_outlier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Upload files to 1950-1959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051\n"
     ]
    }
   ],
   "source": [
    "!ls \"CLEANSED/1950-1959\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "{'2009'}\n"
     ]
    }
   ],
   "source": [
    "content = {}\n",
    "files = glob.glob('RAW/2010-2019_raw/*.txt')\n",
    "\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = \"\"\n",
    "        flag = False\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            if line.startswith('proquest document id:'):\n",
    "                document_id = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('publication year:'):\n",
    "                year = \"\".join(line.replace(\"\\n\", \"\").split(\":\")[1].split())\n",
    "            \n",
    "            if line.startswith('full text:'):\n",
    "                flag=True\n",
    "                data=line.split('full text:')[1].replace(\"\\n\", \"\")\n",
    "            elif line.strip().startswith(sections):\n",
    "                flag=False\n",
    "            elif flag:\n",
    "                data+=line.replace(\"\\n\", \"\")\n",
    "    content[file_name.split(\"/\")[-1][:-4] + \"_\" + year + \"_\" + document_id] = data.strip()\n",
    "years = []\n",
    "for key, value in content.items():\n",
    "    file_name, year, document_id = key.split(\"_\")\n",
    "    if not 2010<= int(year) <=2019:\n",
    "        years.append(year)\n",
    "print(\"FINISHED\")\n",
    "print(set(years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Moved all the outlier files to the 2000-2009 folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text Files for all yeas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different functions: bigrams,trigrams, convert string to counts, update metadata from csv, data update\n",
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into bigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into trigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    \"\"\"Converts a Counter of n-grams into a dictionary\"\"\"\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_from_csv():\n",
    "    \"\"\"Uses pandas to grab additional metadata fields from a CSV file then adds them to the JSON-L file.\n",
    "    Unused fields can be commented out.\"\"\"\n",
    "    Title = df.loc[identifier, 'Title']\n",
    "    Publicationtitle = df.loc[identifier, 'Publicationtitle']\n",
    "    publicationyear = str(df.loc[identifier, 'Publicationyear'])\n",
    "    DocumentURL = df.loc[identifier, 'DocumentURL']\n",
    "    Fulltext = df.loc[identifier, 'Fulltext']\n",
    "    Links = df.loc[identifier, 'Links']\n",
    "    Section = df.loc[identifier, 'Section']\n",
    "    Publicationsubject = str(df.loc[identifier, 'Publicationsubject'])\n",
    "    ISSN = str(df.loc[identifier, 'ISSN'])\n",
    "    Abstract = df.loc[identifier, 'Abstract']\n",
    "    Publicationinfo = df.loc[identifier, 'Publicationinfo']\n",
    "    Lastupdated = df.loc[identifier, 'Lastupdated']\n",
    "    Placeofpublication = df.loc[identifier, 'Placeofpublication']\n",
    "    Location = df.loc[identifier, 'Location']\n",
    "    Author = df.loc[identifier, 'Author']\n",
    "    Publisher = df.loc[identifier, 'Publisher']\n",
    "    Identifierkeyword = df.loc[identifier, 'Identifierkeyword']\n",
    "    Sourcetype = df.loc[identifier, 'Sourcetype']\n",
    "    ProQuestdocumentID = str(df.loc[identifier, 'ProQuestdocumentID'])\n",
    "    Countryofpublication = df.loc[identifier, 'Countryofpublication']\n",
    "    Languageofpublication = df.loc[identifier, 'Languageofpublication']\n",
    "    Publicationdate = df.loc[identifier, 'Publicationdate']\n",
    "    Subject = df.loc[identifier, 'Subject']\n",
    "    Database = df.loc[identifier, 'Database']\n",
    "    Documenttype = df.loc[identifier, 'Documenttype']              \n",
    "\n",
    "#data.update([   \n",
    "#        ('Title', identifier),\n",
    "#        ('Publicationtitle', identifier),\n",
    "#        ('Publicationyear', Publicationyear),\n",
    "#        ('DocumentURL', DocumentURL),\n",
    "#        ('Fulltext', Fulltext),\n",
    "#        ('Links', Links),\n",
    "#        ('Section', Section),\n",
    "#        ('Publicationsubject', Publicationsubject),\n",
    "#        ('ISSN', ISSN),\n",
    "#        ('Abstract', Abstract),\n",
    "#        ('Publicationinfo', Publicationinfo),\n",
    "#        ('Lastupdated', Lastupdated),\n",
    "#        ('Placeofpublication', Placeofpublication),\n",
    "#        ('Location', Location),\n",
    "#        ('Author', Author),\n",
    "#        ('Publisher', Publisher),\n",
    "#        ('Identifierkeyword', Identifierkeyword),\n",
    "#        ('Sourcetype', Sourcetype),\n",
    "#        ('ProQuestdocumentID', ProQuestdocumentID),\n",
    "#        ('Countryofpublication', Countryofpublication),\n",
    "#        ('Languageofpublication', Languageofpublication),\n",
    "#        ('Publicationdate', Publicationdate),\n",
    "#        ('Subject', Subject),\n",
    "#        ('Database', Database),\n",
    "#        ('Documenttype', identifier),\n",
    "#    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`\n",
    "!rm -r TRANSFORMED/\n",
    "!mkdir TRANSFORMED/\n",
    "!mkdir TRANSFORMED/1950-1959\n",
    "!mkdir TRANSFORMED/1960-1969\n",
    "!mkdir TRANSFORMED/1970-1979\n",
    "!mkdir TRANSFORMED/1980-1989\n",
    "!mkdir TRANSFORMED/1990-1999\n",
    "!mkdir TRANSFORMED/2000-2009\n",
    "!mkdir TRANSFORMED/2010-2019\n",
    "!mkdir TRANSFORMED/2020-2029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************FINISHED********************\n"
     ]
    }
   ],
   "source": [
    "cleansed_folders = ['CLEANSED/1950-1959', 'CLEANSED/1960-1969', 'CLEANSED/1970-1979', 'CLEANSED/1980-1989', \n",
    "           'CLEANSED/1990-1999', 'CLEANSED/2000-2009','CLEANSED/2010-2019', 'CLEANSED/2020-2029']\n",
    "punctuations = ['\"\"','\"', \"''\",\"``\",\",\",\".\",\"'\",\";\",\":\",\"[\",\"]\",\"(\",\")\",\"^\", \"{\",\"}\",\"=\",\n",
    "                \"<\",\">\",\"!\",\"/\",\"?\",\"+\",\"|\",\"-\",\"_\",\"%\",\"*\"] + list(string.punctuation)\n",
    "# stopwords from nltk\n",
    "stop_words = list(map(lambda x: x.lower(), set(stopwords.words('english'))))\n",
    "# https://gist.github.com/anubsinha/e65538585a5630a936a426667a807269\n",
    "# https://github.com/bahamas10/prepositions/blob/master/prepositions.json\n",
    "prepositions = ['a', 'abaft', 'aboard', 'about', 'above', 'absent', 'across', 'afore', 'after', 'against', 'along', \n",
    "                'alongside', 'amid', 'amidst', 'among', 'amongst', 'an', 'anenst', 'apropos', 'apud', 'around', 'as', \n",
    "                'aside', 'astride', 'at', 'athwart', 'atop', 'barring', 'before', 'behind', 'below', 'beneath', 'beside', \n",
    "                'besides', 'between', 'beyond', 'but', 'by', 'circa', 'concerning', 'despite', 'down', 'during', 'except', \n",
    "                'excluding', 'failing', 'following', 'for', 'forenenst', 'from', 'given', 'in', 'including', 'inside', \n",
    "                'into', 'is', 'lest', 'like', 'mid', 'midst', 'minus', 'modulo', 'near', 'next', 'notwithstanding', 'of', \n",
    "                'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'pace', 'past', 'per', 'plus', 'pro', 'qua', \n",
    "                'regarding', 'round', 'sans', 'save', 'since', 'than', 'the', 'through', 'throughout', 'till', 'times', \n",
    "                'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'unto', 'up', 'upon', 'versus', 'via', \n",
    "                'vice', 'with', 'within', 'without', 'worth']\n",
    "# https://gist.github.com/mohataher/837a1ed91aab7ab6c8321a2bae18dc3e\n",
    "# https://github.com/witch-house/pronoun.is/blob/master/resources/pronouns.tab\n",
    "pronouns = ['he', 'her', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'it', 'its', 'itself', 'mine', 'my', 'myself', \n",
    "            'our', 'ours', 'ourselves', 'she', 'their', 'theirs', 'them', 'themself', 'themselves', 'they', 'us', 'we', \n",
    "            'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "# https://github.com/first20hours/google-10000-english/blob/master/google-10000-english-usa-no-swears-short.txt\n",
    "common_words = ['huge', 'fly', 'usgs', 'lies', 'thee', 'smtp', 'tb', 'band', 'bulk', 'mood', 's', 'cork', 'mo', 'stay', \n",
    "                'ada', 'had', 'com', 'wear', 'dot', 'cz', 'eur', 'corp', 'fan', 'jp', 'sam', 'fuel', 'dc', 'fast', \n",
    "                'was', 'past', 'stem', 'big', 'bios', 'use', 'rf', 'fcc', 'gr', 'fg', 'body', 'sur', 'sell', 'knit', \n",
    "                'jobs', 'saw', 'sky', 'alt', 'pg', 'well', 'reid', 'ya', 'ed', 'hull', 'la', 'turn', 'line', 'fits', \n",
    "                'yn', 'van', 'pass', 'phil', 'lady', 'ours', 'five', 'bbs', 'dig', 'rosa', 'whom', 'cold', 'tips', \n",
    "                'dumb', 'via', 'km', 'rome', 'iraq', 'est', 'ink', 'hub', 'neon', 'ten', 'sm', 'sept', 'the', 'asn', \n",
    "                'pas', 'junk', 'weed', 'fire', 'web', 'bomb', 'inch', 'gang', 'buy', 'beer', 'cad', 'pr', 'mv', 'jj', \n",
    "                'lows', 'land', 'ryan', 'c', 'fit', 'disk', 'ssl', 'eve', 'ati', 'look', 'cave', 'alto', 'xml', 'cr', \n",
    "                'sec', 'coat', 'node', 'flip', 'oh', 'da', 'pro', 'sand', 'h', 'dk', 'laid', 'own', 'z', 'fix', 'prep', \n",
    "                'ser', 'wc', 'army', 'arms', 'ef', 'mud', 'st', 'kb', 'boot', 'dpi', 'find', 'l', 'lose', 'wb', 'utc', \n",
    "                'is', 'kits', 'ie', 'jill', 'zero', 'cat', 'ta', 'opt', 'faq', 'sen', 'uv', 'th', 'ons', 'rx', 'hist', \n",
    "                'eds', 'dept', 'yrs', 'cal', 'rm', 'arm', 'les', 'md', 'oz', 'dirt', 'us', 'bowl', 'mens', 'talk', 'rr', \n",
    "                'pays', 'bids', 'lace', 'mars', 'doc', 'kept', 'bass', 'dsc', 'jack', 'dts', 'unto', 'der', 'see', 'file', \n",
    "                'once', 'gary', 'sas', 'yea', 'wife', 'ian', 'dame', 'cho', 'toy', 'bmw', 'take', 'gg', 'down', 'ben', \n",
    "                'ur', 'poor', 'sims', 'room', 'aug', 'glad', 'ace', 'spin', 'muze', 'beta', 'fans', 'ntsc', 'vip', 'day', \n",
    "                'pgp', 'mw', 'pie', 'got', 'kyle', 'mf', 't', 'bt', 'buck', 'ca', 'born', 'age', 'rl', 'todd', 'ana', \n",
    "                'href', 'guam', 'jake', 'pal', 'year', 'host', 'font', 'aud', 'feed', 'vg', 'jm', 'nice', 'park', 'jpeg', \n",
    "                'li', 'harm', 'vcr', 'visa', 'isa', 'send', 'bd', 'glen', 'exec', 'pads', 'jose', 'jump', 'chip', 'wow', \n",
    "                'e', 'very', 'fl', 'tr', 'good', 'mary', 'zone', 'key', 'jr', 'pf', 'ease', 'temp', 'earl', 'q', 'cite', \n",
    "                'hold', 'voip', 'jan', 'test', 'polo', 'lg', 'memo', 'dome', 'oval', 'rn', 'be', 'cn', 'dm', 'with', \n",
    "                'nose', 'rat', 'eva', 'ave', 'pe', 'flu', 'hope', 'duck', 'aj', 'pet', 'doe', 'af', 'gets', 'tape', \n",
    "                'ski', 'pan', 'sofa', 'seem', 'sara', 'pest', 'ia', 'cams', 'oman', 'eos', 'hart', 'tion', 'deny', \n",
    "                'gage', 'bond', 'kate', 'adds', 'mb', 'yarn', 'thou', 'echo', 'trap', 'live', 'lord', 'nl', 'amy', \n",
    "                'mpg', 'sci', 'dive', 'hey', 'sent', 'rom', 'now', 'week', 'side', 'due', 'do', 'grew', 'per', 'hiv', \n",
    "                'del', 'mind', 'kong', 'mods', 'lone', 'il', 'inc', 'des', 'msg', 'gzip', 'aged', 'east', 'tall', 'rap', \n",
    "                'ni', 'jews', 'debt', 'biz', 'ste', 'snow', 'wi', 'viii', 'air', 'clay', 'lion', 'die', 'leaf', 'chat', \n",
    "                'hose', 'shoe', 'bit', 'dial', 'ray', 'll', 'blah', 'next', 'been', 'apps', 'only', 'utah', 'son', 'what', \n",
    "                'uri', 'cia', 'sign', 'reef', 'bag', 'oclc', 'zoom', 'rich', 'ist', 'does', 'logs', 'que', 'nm', 'mpeg', \n",
    "                'tech', 'holy', 've', 'duo', 'im', 'hint', 'its', 'liz', 'put', 'mono', 'mall', 'vpn', 'r', 'lat', 'dee', \n",
    "                'ld', 'vt', 'nick', 'gpl', 'tmp', 'hawk', 'tabs', 'bin', 'mark', 'firm', 'ghz', 'gst', 'bo', 'para', \n",
    "                'drew', 'tv', 'ford', 'api', 'neo', 'egg', 'pour', 'ut', 'epa', 'out', 'each', 'bull', 'cas', 'ss', 'mini', \n",
    "                'foot', 'exp', 'jpg', 'spas', 'joe', 'main', 'hans', 'gba', 'nb', 'lip', 'coin', 'tom', 'near', 'v', 're', \n",
    "                'tags', 'inn', 'cord', 'wild', 'aa', 'feof', 'alot', 'mid', 'gray', 'i', 'guys', 'abc', 'gsm', 'ne', 'off', \n",
    "                'llc', 'warm', 'glow', 'sc', 'over', 'fx', 'rip', 'yo', 'sure', 'docs', 'luke', 'halo', 'four', 'pink', \n",
    "                'amp', 'tale', 'head', 'atom', 'pct', 'bp', 'bias', 'such', 'nike', 'tube', 'many', 'seen', 'fail', 'gulf', \n",
    "                'abu', 'rp', 'ml', 'pat', 'gas', 'vhs', 'sw', 'hwy', 'html', 'xbox', 'full', 'bi', 'qc', 'grow', 'tea', \n",
    "                'cup', 'jim', 'wan', 'fm', 'cnet', 'rope', 'nuke', 'adsl', 'ibm', 'sys', 'aaa', 'jvc', 'stop', 'cups', \n",
    "                'sql', 'nz', 'nhs', 'deep', 'neil', 'le', 'evil', 'mls', 'libs', 'ds', 'sale', 'fax', 'obj', 'bent', \n",
    "                'arts', 'ky', 'wa', 'come', 'arch', 'eric', 'hd', 'vids', 'rt', 'drum', 'belt', 'oak', 'game', 'tin', \n",
    "                'hair', 'dump', 'math', 'duty', 'cuba', 'peru', 'crew', 'ppc', 'mad', 'fig', 'burn', 'den', 'chi', \n",
    "                'dem', 'oo', 'laws', 'bali', 'felt', 'busy', 'pi', 'self', 'swap', 'bay', 'une', 'grab', 'ages', 'dow', \n",
    "                'wind', 'semi', 'tba', 'au', 'dist', 'nato', 'box', 'nut', 'caps', 'crop', 'nfl', 'om', 'want', 'edit', \n",
    "                'az', 'iron', 'tee', 'las', 'dont', 'dude', 'id', 'sony', 'ali', 'wu', 'held', 'rpm', 'comp', 'ips', \n",
    "                'two', 'fist', 'lock', 'lift', 'nc', 'jul', 'beds', 'owns', 'home', 'my', 'peak', 'wet', 'rice', 'ruby', \n",
    "                'dash', 'en', 'john', 'loop', 'zoo', 'ice', 'bars', 'sega', 'pn', 'case', 'cod', 'ones', 'fred', 'sv', \n",
    "                'cos', 'no', 'ct', 'lf', 'san', 'tank', 'blue', 'ad', 'boom', 'lime', 'anti', 'hb', 'usda', 'dust', 'she', \n",
    "                'bug', 'lane', 'surf', 'joy', 'urls', 'sub', 'bet', 'ns', 'tu', 'gaps', 'why', 'map', 'marc', 'mix', \n",
    "                'keep', 'dvds', 'um', 'gmc', 'cent', 'wv', 'fy', 'wine', 'tied', 'eye', 'mit', 'leg', 'have', 'lack', \n",
    "                'gold', 'swim', 'qui', 'bite', 'hire', 'ups', 'pl', 'sink', 'zinc', 'rug', 'eat', 'seat', 'ha', 'ids', \n",
    "                'hole', 'aye', 'j', 'upon', 'or', 'bad', 'bare', 'dies', 'vi', 'bee', 'feet', 'cvs', 'pack', 'cash', 'ks', \n",
    "                'lean', 'span', 'maps', 'ind', 'gig', 'suse', 'call', 'wed', 'cove', 'poly', 'sip', 'hit', 'soup', 'np', \n",
    "                'dec', 'dog', 'fw', 'watt', 'sea', 'tc', 'soma', 'spot', 'mia', 'arc', 'bind', 'am', 'tony', 'bio', 'aus', \n",
    "                'ah', 'boys', 'pole', 'toll', 'date', 'cake', 'clip', 'kurt', 'ja', 'pins', 'icq', 'jets', 'tn', 'fear', \n",
    "                'col', 'ripe', 'doug', 'eh', 'es', 'chan', 'sa', 'rh', 'spy', 'tm', 'mat', 'dx', 'lake', 'flat', 'dt', \n",
    "                'some', 'scan', 'thai', 'hook', 'et', 'lo', 'draw', 'reed', 'bone', 'wage', 'rb', 'fate', 'plot', 'knew', \n",
    "                'rc', 'bat', 'cu', 'add', 'ez', 'mins', 'bc', 'dale', 'arg', 'fact', 'when', 'med', 'ny', 'joan', 'lou', \n",
    "                'path', 'hs', 'oc', 'atm', 'baby', 'pics', 'leon', 'sr', 'owen', 'sk', 'lcd', 'mic', 'dock', 'lawn', 'wp', \n",
    "                'g', 'push', 'ps', 'pci', 'tool', 'flux', 'rico', 'help', 'php', 'rim', 'ugly', 'gays', 'lite', 'sf', \n",
    "                'tgp', 'seo', 'foul', 'dos', 'lamp', 'enb', 'tel', 'boat', 'rage', 'nba', 'face', 'nano', 'step', 'tie', \n",
    "                'tone', 'yang', 'lung', 'fine', 'rss', 'view', 'blow', 'load', 'crm', 'wire', 'bg', 'vote', 'free', \n",
    "                'fuji', 'role', 'jury', 'true', 'bugs', 'disc', 'cnn', 'pac', 'lets', 'bra', 'hits', 'mali', 'show', \n",
    "                'neck', 'sail', 'pull', 'mill', 'pt', 'tri', 'gift', 'port', 'mint', 'midi', 'ways', 'same', 'erik', \n",
    "                'an', 'oops', 'nh', 'lazy', 'ball', 'acre', 'eg', 'bed', 'ai', 'divx', 'note', 'fc', 'sl', 'fiji', 'jon', \n",
    "                'drop', 'arab', 'twin', 'acne', 'type', 'img', 'tf', 'gs', 'spec', 'cw', 'wait', 'und', 'jc', 'nv', 'mice', \n",
    "                'loan', 'love', 'team', 'mart', 'dl', 'hp', 'won', 'trip', 'tour', 'bell', 'town', 'told', 'cart', 'carl', \n",
    "                'wax', 'lc', 'ware', 'biol', 'me', 'desk', 'pete', 'uw', 'pace', 'pork', 'dad', 'ka', 'sans', 'io', 'heel', \n",
    "                'gay', 'ev', 'eau', 'rock', 'base', 'hon', 'psi', 'spam', 'rate', 'zip', 'okay', 'fin', 'misc', 'fund', \n",
    "                'suit', 'him', 'miss', 'kim', 'jam', 'kent', 'dev', 'rush', 'mi', 'sat', 'ann', 'df', 'ag', 'mj', 'thu', \n",
    "                'fee', 'palm', 'pub', 'mate', 'cost', 'barn', 'star', 'cpu', 'solo', 'pit', 'pale', 'ic', 'pam', 'hack', \n",
    "                'cats', 'meta', 'road', 'nova', 'yet', 'byte', 'kai', 'calm', 'used', 'man', 'sao', 'boss', 'hunt', 'lp', \n",
    "                'a', 'ide', 'dj', 'pump', 'cool', 'milk', 'edge', 'slow', 'fog', 'nat', 'teen', 'wake', 'soon', 'bold', \n",
    "                'form', 'hugh', 'on', 'wall', 'inf', 'fo', 'sue', 'hugo', 'usd', 'acm', 'proc', 'all', 'dan', 'oven', 'ads', \n",
    "                'dh', 'bid', 'vice', 'mph', 'pa', 'by', 'mode', 'os', 'beam', 'min', 'made', 'mag', 'say', 'gt', 'race', \n",
    "                'wiki', 'exam', 'treo', 'grad', 'rep', 'ltd', 'lt', 'bolt', 'rj', 'fda', 'nt', 'op', 'nest', 'wool', 'text', \n",
    "                'gcc', 'shop', 'mae', 'navy', 'cute', 'drug', 'demo', 'incl', 'asin', 'acts', 'ing', 'max', 'mile', 'zen', \n",
    "                'sole', 'has', 'blog', 'zus', 'lab', 'fake', 'sim', 'u', 'till', 'cure', 'bk', 'aw', 'hop', 'par', 'pmc', \n",
    "                'uni', 'casa', 'ho', 'rg', 'cd', 'bs', 'cube', 'cast', 'poll', 'apr', 'una', 'rule', 'eu', 'ba', 'usps', \n",
    "                'n', 'penn', 'kind', 'and', 'pm', 'sir', 'cet', 'dvd', 'gbp', 'bang', 'fall', 'o', 'vs', 'fa', 'tap', 'gps', \n",
    "                'sort', 'bob', 'his', 'isbn', 'wise', 'chad', 'dr', 'he', 'ye', 'weak', 'uses', 'mlb', 'bald', 'oral', 'xp', \n",
    "                'cop', 'scsi', 'rid', 'rest', 'erp', 'ou', 'di', 'rna', 'page', 'ty', 'rpg', 'plc', 'work', 'pay', 'yeah', \n",
    "                'pick', 'mine', 'nsw', 'shot', 'reel', 'not', 'sb', 'lamb', 'cm', 'grey', 'cruz', 'wins', 'zu', 'low', 'yu', \n",
    "                'pvc', 'rod', 'doom', 'kid', 'gui', 'shaw', 'tail', 'bon', 'win', 'dead', 'wash', 'josh', 'gene', 'quit', \n",
    "                'save', 'gods', 'sie', 'oct', 'reno', 'pty', 'jar', 'dogs', 'ci', 'fwd', 'your', 'hash', 'but', 'keys', \n",
    "                'gis', 'tiny', 'hour', 'go', 'term', 'hip', 'we', 'pda', 'tx', 'rows', 'phd', 'fell', 'http', 'kiss', 'iv', \n",
    "                'auto', 'buf', 'rand', 'vary', 'tft', 'gif', 'fd', 'in', 'pst', 'soap', 'pin', 'var', 'you', 'bar', 'eval', \n",
    "                'hz', 'mail', 'ala', 'ira', 'site', 'folk', 'tim', 'og', 'gd', 'idle', 'sold', 'back', 'kg', 'sg', 'eden', \n",
    "                'eyes', 'mom', 'roy', 'rrp', 'pdt', 'ata', 'sum', 'ipaq', 'isle', 'from', 'top', 'mega', 'prot', 'foo', \n",
    "                'blvd', 'luck', 'nu', 'cfr', 'may', 'cape', 'worm', 'hl', 'poem', 'maui', 'null', 'at', 'zope', 'make', \n",
    "                'quad', 'copy', 'can', 'ga', 'bank', 'gore', 'acer', 'hall', 'nov', 'pd', 'rats', 'know', 'sq', 'need', \n",
    "                'pc', 'guy', 'qty', 'ken', 'cafe', 'tide', 'cb', 'mike', 'west', 'ciao', 'len', 'fame', 'aid', 'lisa', \n",
    "                'ncaa', 'mh', 'sync', 'dual', 'java', 'nor', 'bus', 'urw', 'wifi', 'p', 'new', 'cms', 'else', 'ill', \n",
    "                'thus', 'joke', 'ln', 'her', 'oils', 'mem', 'ccd', 'gc', 'war', 'tons', 'york', 'soc', 'quiz', 'nam', \n",
    "                'law', 'gmbh', 'sing', 'ea', 'res', 'm', 'fees', 'word', 'ak', 'tire', 'pmid', 'carb', 'gym', 'moon', \n",
    "                'beth', 'ant', 'hear', 'last', 'legs', 'gi', 'lynn', 'wt', 'er', 'ham', 'trim', 'ph', 'met', 'nr', 'info', \n",
    "                'lips', 'ping', 'lb', 'logo', 'thru', 'ends', 'ri', 'said', 'like', 'pool', 'jazz', 'dir', 'spa', 'keno', \n",
    "                'con', 'tend', 'gmt', 'read', 'item', 'ra', 'vat', 'rent', 'bm', 'vb', 'ws', 'vast', 'ix', 'wave', 'goto', \n",
    "                'six', 'adam', 'set', 'rob', 'lay', 'clan', 'cio', 'post', 'tent', 'slip', 'bool', 'user', 'asks', 'rw', \n",
    "                'wx', 'duke', 'ext', 'ro', 'xi', 'nn', 'vc', 'str', 'ob', 'dave', 'bags', 'walk', 'ear', 'odd', 'lol', \n",
    "                'shed', 'mar', 'cuts', 'pope', 'done', 'se', 'fool', 'lil', 'psp', 'pop', 'qt', 'lan', 'foto', 'mc', 'aka', \n",
    "                'sn', 'judy', 'fare', 'sage', 'laos', 'wr', 'ron', 'ol', 'chen', 'aqua', 'vic', 'tear', 'mean', 'de', 'gnu', \n",
    "                'pre', 'dsl', 'std', 'size', 'worn', 'iowa', 'wm', 'uk', 'list', 'eyed', 'andy', 'went', 'more', 'pros', \n",
    "                'mil', 'than', 'kde', 'sets', 'stat', 'meal', 'ti', 'pic', 'grid', 'lm', 'mass', 'aims', 'babe', 'por', \n",
    "                'none', 'troy', 'earn', 'rare', 'dui', 'thy', 'wy', 'july', 'aim', 'src', 'el', 'life', 'chem', 'tan', \n",
    "                'ton', 'audi', 'try', 'goal', 'gp', 'play', 'club', 'seed', 'sick', 'frog', 'em', 'yale', 'slim', 'qld', \n",
    "                'wide', 'peer', 'leu', 'dns', 'sake', 'ab', 'jet', 'it', 'cry', 'dice', 'ot', 'sol', 'upc', 'asp', 'shut', \n",
    "                'wood', 'icon', 'as', 'job', 'bush', 'undo', 'sons', 'bb', 'mhz', 'gave', 'd', 'mere', 'cdt', 'ride', \n",
    "                'app', 'red', 'cf', 'hats', 'b', 'mr', 'kay', 'here', 'pair', 'girl', 'norm', 'iran', 'pdf', 'aol', 'pk', \n",
    "                'thin', 'grip', 'bath', 'cars', 'cage', 'ict', 'did', 'ban', 'acc', 'dans', 'slot', 'sep', 'cg', 'kit', \n",
    "                'give', 'das', 'inns', 'f', 'toys', 'tue', 'seek', 'dark', 'ctrl', 'horn', 'gear', 'gl', 'vol', 'roof', \n",
    "                'up', 'org', 'mon', 'dawn', 'doll', 'row', 'usc', 'av', 'jeff', 'je', 'bike', 'wto', 'oem', 'pray', 'los', \n",
    "                'til', 'co', 'hq', 'gap', 'soul', 'nasa', 'ward', 'mas', 'walt', 'tvs', 'long', 'funk', 'half', 'pen', \n",
    "                'pens', 'hood', 'ment', 'that', 'plan', 'cbs', 'sox', 'pos', 'hh', 'pure', 'cole', 'salt', 'sms', 'asus', \n",
    "                'our', 'flow', 'odds', 'ever', 'hu', 'pdas', 'pix', 'pike', 'ash', 'cir', 'mx', 'trio', 'deck', 'lang', \n",
    "                'ii', 'book', 'hdtv', 'irs', 'beef', 'yes', 'root', 'tex', 'vid', 'male', 'xl', 'mask', 'data', 'benz', \n",
    "                'avi', 'wav', 'yard', 'lots', 'hr', 'eq', 'oil', 'hide', 'css', 'snap', 'bbc', 'rec', 'bras', 'camp', \n",
    "                'poet', 'mess', 'best', 'code', 'cds', 'card', 'so', 'are', 'pcs', 'yen', 'ago', 'jay', 'link', 'pj', \n",
    "                'king', 'rose', 'deer', 'bill', 'even', 'diy', 'www', 'pad', 'ins', 'nil', 'plug', 'nec', 'old', 'tune', \n",
    "                'labs', 'ht', 'anna', 'jean', 'door', 'wars', 'ooo', 'llp', 'cs', 'leo', 'raw', 'pod', 'ru', 'su', 'meat', \n",
    "                'pipe', 'soil', 'lbs', 'few', 'du', 'lid', 'onto', 'goat', 'mtv', 'isp', 'act', 'area', 'tt', 'gem', 'toe', \n",
    "                'plus', 'ko', 'lost', 'avg', 'if', 'most', 'wrap', 'usa', 'news', 'eggs', 'dry', 'ion', 'pb', 'sh', 'tp', \n",
    "                'chef', 'gun', 'ec', 'ap', 'hi', 'ate', 'guns', 'moss', 'boc', 'less', 'int', 'vp', 'rs', 'ship', 'real', \n",
    "                'comm', 'rear', 'dat', 'ohio', 'cgi', 'mesh', 'mold', 'ipod', 'heat', 'hero', 'espn', 'von', 'bl', 'rica', \n",
    "                'eng', 'fish', 'msn', 'gm', 'sad', 'tray', 'end', 'brad', 'uc', 'art', 'nbc', 'noon', 'ch', 'song', 'hurt', \n",
    "                'gb', 'open', 'taxi', 'oecd', 'were', 'dear', 'dom', 'dip', 'axis', 'car', 'cl', 'herb', 'gate', 'sp', \n",
    "                'will', 'hill', 'mu', 'rv', 'cv', 'net', 'acid', 'usb', 'pp', 'sri', 'tub', 'bend', 'fri', 'jun', 'gone', \n",
    "                'wit', 'kick', 'bear', 'zum', 'runs', 'phi', 'tier', 'ted', 'w', 'golf', 'uh', 'cab', 'lib', 'bw', 'sees', \n",
    "                'jail', 'guru', 'ebay', 'dis', 'msie', 'gain', 'meet', 'hung', 'tip', 'able', 'iii', 'pond', 'def', 'avon', \n",
    "                'lead', 'nw', 'era', 'urge', 'tile', 'hong', 'hot', 'sean', 'rail', 'ppm', 'mrna', 'prev', 'non', 'epic', \n",
    "                'flag', 'get', 'fed', 'took', 'ge', 'hat', 'pose', 'ruth', 'gen', 'karl', 'task', 'ross', 'gras', 'much', \n",
    "                'feb', 'ram', 'skin', 'nj', 'part', 'bean', 'seq', 'greg', 'cut', 'val', 'mba', 'let', 'loss', 'rugs', \n",
    "                'ring', 'tops', 'mm', 'cst', 'fork', 'rise', 'kw', 'lap', 'fp', 'fat', 'kill', 'drag', 'nuts', 'must', \n",
    "                'join', 'mt', 'how', 'nhl', 'paul', 'lie', 'nyc', 'ip', 'loud', 'pets', 'stan', 'jane', 'cant', 'wal', \n",
    "                'of', 'roll', 'lu', 'ww', 'bow', 'ui', 'both', 'te', 'also', 'mats', 'tax', 'mac', 'menu', 'hay', 'sic', \n",
    "                'euro', 'edt', 'puts', 'nd', 'cam', 'conf', 'sku', 'cult', 'issn', 'fe', 'feat', 'jade', 'fur', 'ft', \n",
    "                'pee', 'sig', 'hang', 'move', 'url', 'ar', 'them', 'unix', 'db', 'geo', 'risk', 'raid', 'lee', 'jd', 'ep', \n",
    "                'gale', 'too', 'cdna', 'stud', 'rio', 'ts', 'ties', 'irc', 'hand', 'ieee', 'mod', 'ls', 'pine', 'anne', 'mg', \n",
    "                'ul', 'easy', 'nav', 'dg', 'pts', 'alex', 'goes', 'cc', 'lens', 'k', 'cj', 'usr', 'gel', 'log', 'ok', 'ftp', \n",
    "                'dv', 'exit', 'peas', 'phys', 'mel', 'nine', 'rd', 'dose', 'foam', 'way', 'na', 'diet', 'away', 'rfc', 'fort', \n",
    "                'oe', 'late', 'po', 'says', 'dna', 'don', 'jeep', 'film', 'eco', 'dish', 'tell', 'yoga', 'run', 'wn', 'va', \n",
    "                'alan', 'cap', 'soa', 'for', 'wolf', 'ng', 'div', 'dp', 'ma', 'paso', 'just', 'cell', 'gdp', 'feel', 'ears', \n",
    "                'fi', 'paid', 'pubs', 'hk', 'si', 'silk', 'deaf', 'dd', 'wing', 'luis', 'bits', 'ent', 'td', 'mp', 'mime', \n",
    "                'emma', 'pig', 'rack', 'left', 'knee', 'cow', 'nail', 'ac', 'sit', 'rca', 'apt', 'hc', 'ee', 'gtk', 'dean', \n",
    "                'sap', 'fbi', 'ce', 'time', 'abs', 'ms', 'sd', 'food', 'kids', 'etc', 'dam', 'corn', 'sin', 'hard', 'led', \n",
    "                'died', 'rev', 'skip', 'cook', 'ver', 'rick', 'men', 'beat', 'buzz', 'univ', 'idea', 'kirk', 'faqs', 'dana', \n",
    "                'mn', 'za', 'mrs', 'joel', 'intl', 'safe', 'lit', 'mug', 'lot', 'then', 'br', 'buys', 'perl', 'aids', 'flex', \n",
    "                'keen', 'ir', 'fr', 'char', 'hate', 'mesa', 'high', 'ask', 'dod', 'tab', 'rays', 'iso', 'ran', 'cons', \n",
    "                'expo', 'prix', 'seal', 'core', 'sun', 'fun', 'amd', 'hrs', 'pill', 'trek', 'loc', 'al', 'yr', 'dell', 'who', \n",
    "                'fair', 'juan', 'mild', 'any', 'pot', 'tcp', 'dare', 'ceo', 'one', 'dim', 'city', 'tar', 'fill', 'pain', \n",
    "                'name', 'into', 'un', 'volt', 'ex', 'matt', 'to', 'vii', 'farm', 'fold', 'js', 'rank', 'rely', 'oaks', \n",
    "                'geek', 'seas', 'idol', 'days', 'fu', 'reg', 'levy', 'punk', 'ae', 'coal', 'ddr', 'fs', 'x', 'care', 'lucy', \n",
    "                'unit', 'june', 'ff', 'asia', 'came', 'bye', 'boy', 'y', 'myth', 'bird', 'moms', 'this', 'cp', 'wish', \n",
    "                'rain', 'wma', 'fox', 'mai', 'diff', 'deal', 'cope', 'they', 'void', 'ref', 'far', 'tree', 'soft', 'tag']\n",
    "# final list\n",
    "stop = stop_words + pronouns + prepositions + common_words\n",
    "\n",
    "for folder in cleansed_folders:\n",
    "    files = glob.glob(folder+'/*.txt')\n",
    "    for file_name in files:\n",
    "        with open(file_name, 'r') as f:\n",
    "            data = f.read().lower()\n",
    "            # remove special characters\n",
    "            data = re.sub(r'[^\\x00-\\x7f]',r'', data)\n",
    "            # remove some punctuations\n",
    "            for punctuation in punctuations:\n",
    "                data = data.replace(punctuation, \"\")\n",
    "            # remove stop words, one word characters, and punctuations\n",
    "            data = ' '.join([i for i in data.split() if (len(i)>2 or i=='ai') and (i not in stop or i=='ai')])\n",
    "            # fix spaces\n",
    "            data = data.strip()\n",
    "            data = re.sub(r'\\s+',r' ', data)\n",
    "        with open(\"TRANSFORMED/\"+folder.split(\"/\")[1]+\"/\"+file_name.split(\"/\")[-1], 'w') as file:\n",
    "            file.write(data)\n",
    "print(\"*\"*20+\"FINISHED\"+\"*\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051 items written to TRANSFORMED/1950-1959.jsonl.\n",
      "2900 items written to TRANSFORMED/1960-1969.jsonl.\n",
      "1258 items written to TRANSFORMED/1970-1979.jsonl.\n",
      "1771 items written to TRANSFORMED/1980-1989.jsonl.\n",
      "1520 items written to TRANSFORMED/1990-1999.jsonl.\n",
      "2010 items written to TRANSFORMED/2000-2009.jsonl.\n",
      "2852 items written to TRANSFORMED/2010-2019.jsonl.\n",
      "412 items written to TRANSFORMED/2020-2029.jsonl.\n"
     ]
    }
   ],
   "source": [
    "transformed_folders = ['TRANSFORMED/1950-1959', 'TRANSFORMED/1960-1969', 'TRANSFORMED/1970-1979', 'TRANSFORMED/1980-1989', \n",
    "           'TRANSFORMED/1990-1999', 'TRANSFORMED/2000-2009','TRANSFORMED/2010-2019', 'TRANSFORMED/2020-2029']\n",
    "\n",
    "for corpus_root in transformed_folders:\n",
    "    # Creating corpus using all the text files in root folder\n",
    "    corpus = PlaintextCorpusReader(corpus_root, '.*txt')\n",
    "    # Print all File IDs in corpus based on text file names ###\n",
    "    text_list = corpus.fileids()\n",
    "    # Define the file output name\n",
    "    output_filename = corpus_root+'.jsonl'\n",
    "\n",
    "    for text in text_list:\n",
    "        # Create identifier from filename\n",
    "        if corpus_root in ['TRANSFORMED/1950-1959', 'TRANSFORMED/1960-1969', 'TRANSFORMED/1970-1979']:\n",
    "            identifier = text.split(\"_\")[0]\n",
    "        else:\n",
    "            identifier = text.split(\"_\")[-1][:-4]\n",
    "\n",
    "        # Compute unigrams\n",
    "        unigrams = corpus.words(text)\n",
    "        unigramCount = convert_strings_to_counts(unigrams)\n",
    "\n",
    "        # Compute bigrams\n",
    "        tuple_bigrams = list(nltk.bigrams(unigrams))\n",
    "        string_bigrams = convert_tuple_bigrams(tuple_bigrams)\n",
    "        bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "\n",
    "        # Compute trigrams\n",
    "        tuple_trigrams = list(nltk.trigrams(unigrams))\n",
    "        string_trigrams = convert_tuple_trigrams(tuple_trigrams)\n",
    "        trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "\n",
    "        # Compute fulltext\n",
    "        with open(corpus_root+'/'+text, 'r') as file:\n",
    "            fullText = file.read()\n",
    "\n",
    "        # Calculate wordCount\n",
    "        wordCount = 0\n",
    "        for counts in unigramCount.values():\n",
    "            wordCount = wordCount + counts\n",
    "\n",
    "        # Create a dictionary `data` to hold each document's data\n",
    "        # Including id, wordCount, outputFormat, unigramCount,\n",
    "        # bigramCount, trigramCount, fullText, etc.\n",
    "        data = {}\n",
    "\n",
    "        data.update([\n",
    "            ('id', identifier),\n",
    "            ('outputFormat', ['unigram', 'bigram', 'trigram', 'fullText']),\n",
    "            ('wordCount', wordCount),\n",
    "            ('fullText', fullText),\n",
    "            ('unigramCount', unigramCount), \n",
    "            ('bigramCount', bigramCount), \n",
    "            ('trigramCount', trigramCount)\n",
    "        ])\n",
    "\n",
    "        # Add additional metadata if there is a metadata.csv available\n",
    "    #     df = pd.read_csv(corpus_root+'.csv')\n",
    "    #     df.set_index('id', inplace=True)\n",
    "    #     # Update Metadata\n",
    "    #     update_metadata_from_csv()\n",
    "\n",
    "\n",
    "        # Write the document to the json file  \n",
    "        with open(output_filename, 'a') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    print(str(len(text_list)) + f' items written to {output_filename}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder CLEANSED/1950-1959 has 1051 files\n",
      "Folder CLEANSED/1960-1969 has 2900 files\n",
      "Folder CLEANSED/1970-1979 has 1258 files\n",
      "Folder CLEANSED/1980-1989 has 1771 files\n",
      "Folder CLEANSED/1990-1999 has 1520 files\n",
      "Folder CLEANSED/2000-2009 has 2010 files\n",
      "Folder CLEANSED/2010-2019 has 2852 files\n",
      "Folder CLEANSED/2020-2029 has 412 files\n",
      "Folder TRANSFORMED/1950-1959 has 1051 files\n",
      "Folder TRANSFORMED/1960-1969 has 2900 files\n",
      "Folder TRANSFORMED/1970-1979 has 1258 files\n",
      "Folder TRANSFORMED/1980-1989 has 1771 files\n",
      "Folder TRANSFORMED/1990-1999 has 1520 files\n",
      "Folder TRANSFORMED/2000-2009 has 2010 files\n",
      "Folder TRANSFORMED/2010-2019 has 2852 files\n",
      "Folder TRANSFORMED/2020-2029 has 412 files\n"
     ]
    }
   ],
   "source": [
    "cleansed_folders = ['CLEANSED/1950-1959', 'CLEANSED/1960-1969', 'CLEANSED/1970-1979', 'CLEANSED/1980-1989', \n",
    "           'CLEANSED/1990-1999', 'CLEANSED/2000-2009','CLEANSED/2010-2019', 'CLEANSED/2020-2029']\n",
    "for folder in cleansed_folders:\n",
    "    files = glob.glob(folder+'/*.txt')\n",
    "    print(\"Folder {} has {} files\".format(folder, len(files)))\n",
    "\n",
    "transformed_folders = ['TRANSFORMED/1950-1959', 'TRANSFORMED/1960-1969', 'TRANSFORMED/1970-1979', 'TRANSFORMED/1980-1989', \n",
    "           'TRANSFORMED/1990-1999', 'TRANSFORMED/2000-2009','TRANSFORMED/2010-2019', 'TRANSFORMED/2020-2029']\n",
    "for folder in transformed_folders:\n",
    "    files = glob.glob(folder+'/*.txt')\n",
    "    print(\"Folder {} has {} files\".format(folder, len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1051+2900+1258+1771+1520+2010+2852+412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.16xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
